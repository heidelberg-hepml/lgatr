torch>=2.6.0  # required for flex-attention
einops
opt_einsum
